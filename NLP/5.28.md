自然语言处理每日论文速递[05.28]
=======

cs.CL 方向，今日共计36篇
=

 
【1】 Combating Adversarial Misspellings with Robust Word Recognition
标题：用强大的单词识别来对抗对抗性拼写错误
作者： Danish Pruthi,  Zachary C. Lipton 
备注：To appear at ACL 2019
链接：https://arxiv.org/abs/1905.11268
摘要： To combat adversarial spelling mistakes, we propose placing a wordrecognition model in front of the downstream classifier. Our word recognitionmodels build upon the RNN semi-character architecture, introducing several newbackoff strategies for handling rare and unseen words. Trained to recognizewords corrupted by random adds, drops, swaps, and keyboard mistakes, our methodachieves 32% relative (and 3.3% absolute) error reduction over the vanillasemi-character model. Notably, our pipeline confers robustness on thedownstream classifier, outperforming both adversarial training andoff-the-shelf spell checkers. Against a BERT model fine-tuned for sentimentanalysis, a single adversarially-chosen character attack lowers accuracy from90.3% to 45.8%. Our defense restores accuracy to 75%. Surprisingly, better wordrecognition does not always entail greater robustness. Our analysis revealsthat robustness also depends upon a quantity that we denote the sensitivity.
 
【2】 AgentGraph: Towards Universal Dialogue Management with Structured Deep  Reinforcement Learning
标题：AgentGraph：通过结构化深层强化学习实现普遍对话管理
作者： Lu Chen,  Kai Yu 
链接：https://arxiv.org/abs/1905.11259
摘要： Dialogue policy plays an important role in task-oriented spoken dialoguesystems. It determines how to respond to users. The recently proposed deepreinforcement learning (DRL) approaches have been used for policy optimization.However, these deep models are still challenging for two reasons: 1) ManyDRL-based policies are not sample-efficient. 2) Most models don't have thecapability of policy transfer between different domains. In this paper, wepropose a universal framework, AgentGraph, to tackle these two problems. Theproposed AgentGraph is the combination of GNN-based architecture and DRL-basedalgorithm. It can be regarded as one of the multi-agent reinforcement learningapproaches. Each agent corresponds to a node in a graph, which is definedaccording to the dialogue domain ontology. When making a decision, each agentcan communicate with its neighbors on the graph. Under AgentGraph framework, wefurther propose Dual GNN-based dialogue policy, which implicitly decomposes thedecision in each turn into a high-level global decision and a low-level localdecision. Experiments show that AgentGraph models significantly outperformtraditional reinforcement learning approaches on most of the 18 tasks of thePyDial benchmark. Moreover, when transferred from the source task to a targettask, these models not only have acceptable initial performance but alsoconverge much faster on the target task.
 
【3】 Bridging Dialogue Generation and Facial Expression Synthesis
标题：弥合对话生成和面部表情综合
作者： Shang-Yu Su,  Yun-Nung Chen 
备注：arXiv admin note: text overlap with arXiv:1807.09251, arXiv:1802.08379 by other authors
链接：https://arxiv.org/abs/1905.11240
摘要： Spoken dialogue systems that assist users to solve complex tasks such asmovie ticket booking have become an emerging research topic in artificialintelligence and natural language processing areas. With a well-designeddialogue system as an intelligent personal assistant, people can accomplishcertain tasks more easily via natural language interactions. Today there areseveral virtual intelligent assistants in the market; however, most systemsonly focus on single modality, such as textual or vocal interaction. Amultimodal interface has various advantages: (1) allowing human to communicatewith machines in a natural and concise form using the mixture of modalitiesthat most precisely convey the intention to satisfy communication needs, and(2) providing more engaging experience by natural and human-like feedback. Thispaper explores a brand new research direction, which aims at bridging dialoguegeneration and facial expression synthesis for better multimodal interaction.The goal is to generate dialogue responses and simultaneously synthesizecorresponding visual expressions on faces, which is also an ultimate steptoward more human-like virtual assistants.
 
【4】 CIF: Continuous Integrate-and-Fire for End-to-End Speech Recognition
标题：CIF：用于端到端语音识别的连续集成和消防
作者： Linhao Dong,  Bo Xu 
链接：https://arxiv.org/abs/1905.11235
摘要： Automatic speech recognition (ASR) system is undergoing an exciting pathwayto be more simplified and practical with the spring up of various end-to-endmodels. However, the mainstream of them neglects the positioning of tokenboundaries from continuous speech, which is considered crucial in humanlanguage learning and instant speech recognition. In this work, we proposeContinuous Integrate-and-Fire (CIF), a 'soft' and 'monotonic'acoustic-to-linguistic alignment mechanism that addresses the boundarypositioning by simulating the integrate-and-fire neuron model using continuousfunctions under the encoder-decoder framework. As the connection between theencoder and decoder, the CIF forwardly integrates the information in theencoded acoustic representations to determine a boundary and instantly firesthe integrated information to the decoder once a boundary is located. Multipleeffective strategies are introduced to the CIF-based model to alleviate theproblems brought by the inaccurate positioning. Besides, multi-task learning isperformed during training and an external language model is incorporated duringinference to further boost the model performance. Evaluated on multiple ASRdatasets that cover different languages and speech types, the CIF-based modelshows stable convergence and competitive performance. Especially, it achieves aword error rate (WER) of 3.70% on the test-clean of Librispeech.
 
【5】 Harry Potter and the Action Prediction Challenge from Natural Language
标题：哈利波特与自然语言的行动预测挑战
作者： David Vilares,  Carlos Gómez-Rodríguez 
备注：NAACL 2019 (short papers)
链接：https://arxiv.org/abs/1905.11037
摘要： We explore the challenge of action prediction from textual descriptions ofscenes, a testbed to approximate whether text inference can be used to predictupcoming actions. As a case of study, we consider the world of the Harry Potterfantasy novels and inferring what spell will be cast next given a fragment of astory. Spells act as keywords that abstract actions (e.g. 'Alohomora' to open adoor) and denote a response to the environment. This idea is used toautomatically build HPAC, a corpus containing 82,836 samples and 85 actions. Wethen evaluate different baselines. Among the tested models, an LSTM-basedapproach obtains the best performance for frequent actions and large scenedescriptions, but approaches such as logistic regression behave well oninfrequent actions.
 
【6】 Levenshtein Transformer
标题：Levenshtein变压器
作者： Jiatao Gu,  Jake Zhao 
链接：https://arxiv.org/abs/1905.11006
摘要： Modern neural sequence generation models are built to either generate tokensstep-by-step from scratch or (iteratively) modify a sequence of tokens boundedby a fixed length. In this work, we develop Levenshtein Transformer, a newpartially autoregressive model devised for more flexible and amenable sequencegeneration. Unlike previous approaches, the atomic operations of our model areinsertion and deletion. The combination of them facilitates not only generationbut also sequence refinement allowing dynamic length changes. We also propose aset of new training techniques dedicated at them, effectively exploiting one asthe other's learning signal thanks to their complementary nature. Experimentsapplying the proposed model achieve comparable performance but much-improvedefficiency on both generation (e.g. machine translation, text summarization)and refinement tasks (e.g. automatic post-editing). We further confirm theflexibility of our model by showing a Levenshtein Transformer trained bymachine translation can straightforwardly be used for automatic post-editing.
 
【7】 Commonsense Properties from Query Logs and Question Anwering Forums
标题：查询日志和问题回答论坛中的常识属性
作者： Julien Romero,  Gerhard Weikum 
链接：https://arxiv.org/abs/1905.10989
摘要： Commonsense knowledge about object properties, human behavior and generalconcepts is crucial for robust AI applications. However, automatic acquisitionof this knowledge is challenging because of sparseness and bias in onlinesources. This paper presents Quasimodo, a methodology and tool suite fordistilling commonsense properties from non-standard web sources. We devisenovel ways of tapping into search-engine query logs and QA forums, andcombining the resulting candidate assertions with statistical cues fromencyclopedias, books and image tags in a corroboration step. Unlike prior workon commonsense knowledge bases, Quasimodo focuses on salient properties thatare typically associated with certain objects or concepts. Extensiveevaluations, including extrinsic use-case studies, show that Quasimodo providesbetter coverage than state-of-the-art baselines with comparable quality.
 
【8】 Extreme Multi-Label Legal Text Classification: A case study in EU  Legislation
标题：极端多标签法律文本分类：欧盟立法案例研究
作者： Ilias Chalkidis,  Ion Androutsopoulos 
备注：10 pages, long paper at NLLP Workshop of NAACL-HLT 2019
链接：https://arxiv.org/abs/1905.10892
摘要： We consider the task of Extreme Multi-Label Text Classification (XMTC) in thelegal domain. We release a new dataset of 57k legislative documents fromEURLEX, the European Union's public document database, annotated with conceptsfrom EUROVOC, a multidisciplinary thesaurus. The dataset is substantiallylarger than previous EURLEX datasets and suitable for XMTC, few-shot andzero-shot learning. Experimenting with several neural classifiers, we show thatBIGRUs with self-attention outperform the current multi-label state-of-the-artmethods, which employ label-wise attention. Replacing CNNs with BIGRUs inlabel-wise attention networks leads to the best overall performance.
 
【9】 Where's My Head? Definition, Dataset and Models for Numeric Fused-Heads  Identification and Resolution
标题：哪里是我的头？数字融合头识别和分辨率的定义，数据集和模型
作者： Yanai Elazar,  Yoav Goldberg 
链接：https://arxiv.org/abs/1905.10886
摘要： We provide the first computational treatment of fused-heads constructions(FH), focusing on the numeric fused-heads (NFH). FHs constructions are nounphrases (NPs) in which the head noun is missing and is said to be `fused' withits dependent modifier. This missing information is implicit and is importantfor sentence understanding. The missing references are easily filled in byhumans but pose a challenge for computational models. We formulate the handlingof FH as a two stages process: identification of the FH construction andresolution of the missing head. We explore the NFH phenomena in large corporaof English text and create (1) a dataset and a highly accurate method for NFHidentification; (2) a 10k examples (1M tokens) crowd-sourced dataset of NFHresolution; and (3) a neural baseline for the NFH resolution task. We releaseour code and dataset, in hope to foster further research into this challengingproblem.
 
【10】 When to reply? Context Sensitive Models to Predict Instructor  Interventions in MOOC Forums
标题：什么时候回复？用于预测MOOC论坛中讲师干预的上下文敏感模型
作者： Muthu Kumar Chandrasekaran,  Min-Yen Kan 
链接：https://arxiv.org/abs/1905.10851
摘要： Due to time constraints, course instructors often need to selectivelyparticipate in student discussion threads, due to their limited bandwidth andlopsided student--instructor ratio on online forums. We propose the first deeplearning models for this binary prediction problem. We propose novel attentionbased models to infer the amount of latent context necessary to predictinstructor intervention. Such models also allow themselves to be tuned toinstructor's preference to intervene early or late. Our three proposedattentive model variants to infer the latent context improve over thestate-of-the-art by a significant, large margin of 11% in F1 and 10% in recall,on average. Further, introspection of attention help us better understand whataspects of a discussion post propagate through the discussion thread thatprompts instructor intervention.
 
【11】 Simple and Effective Curriculum Pointer-Generator Networks for Reading  Comprehension over Long Narratives
标题：简单有效的课程指针生成器网络阅读理解长篇叙事
作者： Yi Tay,  Aston Zhang 
备注：Accepted to ACL 2019
链接：https://arxiv.org/abs/1905.10847
摘要： This paper tackles the problem of reading comprehension over long narrativeswhere documents easily span over thousands of tokens. We propose a curriculumlearning (CL) based Pointer-Generator framework for reading/sampling over largedocuments, enabling diverse training of the neural model based on the notion ofalternating contextual difficulty. This can be interpreted as a form of domainrandomization and/or generative pretraining during training. To this end, theusage of the Pointer-Generator softens the requirement of having the answerwithin the context, enabling us to construct diverse training samples forlearning. Additionally, we propose a new Introspective Alignment Layer (IAL),which reasons over decomposed alignments using block-based self-attention. Weevaluate our proposed method on the NarrativeQA reading comprehensionbenchmark, achieving state-of-the-art performance, improving existing baselinesby $51\%$ relative improvement on BLEU-4 and $17\%$ relative improvement onRouge-L. Extensive ablations confirm the effectiveness of our proposed IAL andCL components.
 
【12】 Evaluation of basic modules for isolated spelling error correction in  Polish texts
标题：评估波兰文本中孤立拼写错误纠正的基本模块
作者： Szymon Rutkowski 
备注：4 pages, LTC '19 Pozna\'n
链接：https://arxiv.org/abs/1905.10810
摘要： Spelling error correction is an important problem in natural languageprocessing, as a prerequisite for good performance in downstream tasks as wellas an important feature in user-facing applications. For texts in Polishlanguage, there exist works on specific error correction solutions, oftendeveloped for dealing with specialized corpora, but not evaluations of manydifferent approaches on big resources of errors. We begin to address thisproblem by testing some basic and promising methods on PlEWi, a corpus ofannotated spelling extracted from Polish Wikipedia. These modules may befurther combined with appropriate solutions for error detection and contextawareness. Following our results, combining edit distance with cosine distanceof semantic vectors may be suggested for interpretable systems, while an LSTM,particularly enhanced by ELMo embeddings, seems to offer the best rawperformance.
 
【13】 TIGS: An Inference Algorithm for Text Infilling with Gradient Search
标题：TIGS：一种基于梯度搜索的文本填充推理算法
作者： Dayiheng Liu,  Jiancheng Lv 
备注：The 57th Annual Meeting of the Association for Computational Linguistics (ACL 2019)
链接：https://arxiv.org/abs/1905.10752
摘要： Text infilling is defined as a task for filling in the missing part of asentence or paragraph, which is suitable for many real-world natural languagegeneration scenarios. However, given a well-trained sequential generativemodel, generating missing symbols conditioned on the context is challenging forexisting greedy approximate inference algorithms. In this paper, we propose aniterative inference algorithm based on gradient search, which is the firstinference algorithm that can be broadly applied to any neural sequencegenerative models for text infilling tasks. We compare the proposed method withstrong baselines on three text infilling tasks with various mask ratios anddifferent mask strategies. The results show that our proposed method iseffective and efficient for fill-in-the-blank tasks, consistently outperformingall baselines.
 
【14】 SemBleu: A Robust Metric for AMR Parsing Evaluation
标题：SemBleu：AMR分析评估的稳健指标
作者： Linfeng Song,  Daniel Gildea 
备注：ACL 2019 camera ready
链接：https://arxiv.org/abs/1905.10726
摘要： Evaluating AMR parsing accuracy involves comparing pairs of AMR graphs. Theonly existing evaluation metric, Smatch (Cai and Knight, 2013), searches forone-to-one mappings between the nodes of two AMRs with a greedy hill-climbingalgorithm, which leads to search errors. We propose SemBleu, a robust metricthat extends BLEU (Papineni et al., 2002) to AMRs. It does not suffer fromsearch errors and considers non-local correspondences in addition to localones. SemBleu is fully content-driven and punishes situations where a systemoutput does not preserve most information from the input. Preliminaryexperiments on both sentence and corpus levels show that SemBleu has slightlyhigher consistency with human judgments than Smatch. Our code and data athttp://github. com/freesunshine0316/sembleu.
 
【15】 Gated Group Self-Attention for Answer Selection
标题：门控小组自我注意答案选择
作者： Dong Xu,  Wu-Jun Li 
链接：https://arxiv.org/abs/1905.10720
摘要： Answer selection (answer ranking) is one of the key steps in many kinds ofquestion answering (QA) applications, where deep models have achievedstate-of-the-art performance. Among these deep models, recurrent neural network(RNN) based models are most popular, typically with better performance thanconvolutional neural network (CNN) based models. Nevertheless, it is difficultfor RNN based models to capture the information about long-range dependencyamong words in the sentences of questions and answers. In this paper, wepropose a new deep model, called gated group self-attention (GGSA), for answerselection. GGSA is inspired by global self-attention which is originallyproposed for machine translation and has not been explored in answer selection.GGSA tackles the problem of global self-attention that local and globalinformation cannot be well distinguished. Furthermore, an interaction mechanismbetween questions and answers is also proposed to enhance GGSA by a residualstructure. Experimental results on two popular QA datasets show that GGSA canoutperform existing answer selection models to achieve state-of-the-artperformance. Furthermore, GGSA can also achieve higher accuracy than globalself-attention for the answer selection task, with a lower computation cost.
 
【16】 Hashing based Answer Selection
标题：基于哈希的答案选择
作者： Dong Xu,  Wu-Jun Li 
链接：https://arxiv.org/abs/1905.10718
摘要： Answer selection is an important subtask of question answering (QA), wheredeep models usually achieve better performance. Most deep models adoptquestion-answer interaction mechanisms, such as attention, to get vectorrepresentations for answers. When these interaction based deep models aredeployed for online prediction, the representations of all answers need to berecalculated for each question. This procedure is time-consuming for deepmodels with complex encoders like BERT which usually have better accuracy thansimple encoders. One possible solution is to store the matrix representation(encoder output) of each answer in memory to avoid recalculation. But this willbring large memory cost. In this paper, we propose a novel method, calledhashing based answer selection (HAS), to tackle this problem. HAS adopts ahashing strategy to learn a binary matrix representation for each answer, whichcan dramatically reduce the memory cost for storing the matrix representationsof answers. Hence, HAS can adopt complex encoders like BERT in the model, butthe online prediction of HAS is still fast with a low memory cost. Experimentalresults on three popular answer selection datasets show that HAS can outperformexisting models to achieve state-of-the-art performance.
 
【17】 Are Sixteen Heads Really Better than One?
标题：十六头真的比一个好吗？
作者： Paul Michel,  Graham Neubig 
链接：https://arxiv.org/abs/1905.10650
摘要： Attention is a powerful and ubiquitous mechanism for allowing neural modelsto focus on particular salient pieces of information by taking their weightedaverage when making predictions. In particular, multi-headed attention is adriving force behind many recent state-of-the-art NLP models such asTransformer-based MT models and BERT. These models apply multiple attentionmechanisms in parallel, with each attention "head" potentially focusing ondifferent parts of the input, which makes it possible to express sophisticatedfunctions beyond the simple weighted average. In this paper we make thesurprising observation that even if models have been trained using multipleheads, in practice, a large percentage of attention heads can be removed attest time without significantly impacting performance. In fact, some layers caneven be reduced to a single head. We further examine greedy algorithms forpruning down models, and the potential speed, memory efficiency, and accuracyimprovements obtainable therefrom. Finally, we analyze the results with respectto which parts of the model are more reliant on having multiple heads, andprovide precursory evidence that training dynamics play a role in the gainsprovided by multi-head attention.
 
【18】 ESA: Entity Summarization with Attention
标题：ESA：注意的实体摘要
作者： Dongjun Wei,  Yaxin Liu 
链接：https://arxiv.org/abs/1905.10625
摘要： Entity summarization aims at creating brief but informative descriptions ofentities from knowledge graphs. While previous work mostly focused ontraditional techniques such as clustering algorithms and graph models, we askhow to apply deep learning methods into this task. In this paper we proposeESA, a neural network with supervised attention mechanisms for entitysummarization. Specifically, we calculate attention weights for facts in eachentity, and rank facts to generate reliable summaries. We explore techniques tosolve difficult learning problems presented by the ESA, and demonstrate theeffectiveness of our model in comparison with the state-of-the-art methods.Experimental results show that our model improves the quality of the entitysummaries in both F-measure and MAP.
 
【19】 Soft Contextual Data Augmentation for Neural Machine Translation
标题：神经机器翻译的软上下文数据增强
作者： Jinhua Zhu,  Tie-Yan Liu 
备注：Accepted by ACL 2019 as short paper
链接：https://arxiv.org/abs/1905.10523
摘要： While data augmentation is an important trick to boost the accuracy of deeplearning methods in computer vision tasks, its study in natural language tasksis still very limited. In this paper, we present a novel data augmentationmethod for neural machine translation. Different from previous augmentationmethods that randomly drop, swap or replace words with other words in asentence, we softly augment a randomly chosen word in a sentence by itscontextual mixture of multiple related words. More accurately, we replace theone-hot representation of a word by a distribution (provided by a languagemodel) over the vocabulary, i.e., replacing the embedding of this word by aweighted combination of multiple semantically similar words. Since the weightsof those words depend on the contextual information of the word to be replaced,the newly generated sentences capture much richer information than previousaugmentation methods. Experimental results on both small scale and large scalemachine translation datasets demonstrate the superiority of our method overstrong baselines.
 
【20】 SuperCaptioning: Image Captioning Using Two-dimensional Word Embedding
标题：SuperCaptioning：使用二维字嵌入的图像字幕
作者： Baohua Sun,  Jason Dong 
链接：https://arxiv.org/abs/1905.10515
摘要： Language and vision are processed as two different modal in current work forimage captioning. However, recent work on Super Characters method shows theeffectiveness of two-dimensional word embedding, which converts textclassification problem into image classification problem. In this paper, wepropose the SuperCaptioning method, which borrows the idea of two-dimensionalword embedding from Super Characters method, and processes the information oflanguage and vision together in one single CNN model. The experimental resultson Flickr30k data shows the proposed method gives high quality image captions.An interactive demo is ready to show at the workshop.
 
【21】 Designing a Symbolic Intermediate Representation for Neural Surface  Realization
标题：设计神经表面实现的符号中间表示
作者： Henry Elder,  Alexander O'Connor 
链接：https://arxiv.org/abs/1905.10486
摘要： Generated output from neural NLG systems often contain errors such ashallucination, repetition or contradiction. This work focuses on designing asymbolic intermediate representation to be used in multi-stage neuralgeneration with the intention of reducing the frequency of failed outputs. Weshow that surface realization from this intermediate representation is of highquality and when the full system is applied to the E2E dataset it outperformsthe winner of the E2E challenge. Furthermore, by breaking out the surfacerealization step from typically end-to-end neural systems, we also provide aframework for non-neural content selection and planning systems to potentiallytake advantage of semi-supervised pretraining of neural surface realizationmodels.
 
【22】 DebiasingWord Embeddings Improves Multimodal Machine Translation
标题：DebiasingWord Embeddings改进了多模态机器翻译
作者： Tosho Hirasawa,  Mamoru Komachi 
备注：10 pages; MT Summit 2019
链接：https://arxiv.org/abs/1905.10464
摘要： In recent years, pretrained word embeddings have proved useful for multimodalneural machine translation (NMT) models to address the shortage of availabledatasets. However, the integration of pretrained word embeddings has not yetbeen explored extensively. Further, pretrained word embeddings in highdimensional spaces have been reported to suffer from the hubness problem.Although some debiasing techniques have been proposed to address this problemfor other natural language processing tasks, they have seldom been studied formultimodal NMT models. In this study, we examine various kinds of wordembeddings and introduce two debiasing techniques for three multimodal NMTmodels and two language pairs -- English-German translation and English-Frenchtranslation. With our optimal settings, the overall performance of multimodalmodels was improved by up to +1.93 BLEU and +2.02 METEOR for English-Germantranslation and +1.73 BLEU and +0.95 METEOR for English-French translation.
 
【23】 A Call for Prudent Choice of Subword Merge Operations
标题：要求谨慎选择子字合并操作
作者： Shuoyang Ding,  Kevin Duh 
备注：Accepted to MT Summit 2019
链接：https://arxiv.org/abs/1905.10453
摘要： Most neural machine translation systems are built upon subword unitsextracted by methods such as Byte-Pair Encoding (BPE) or wordpiece. However,the choice of number of merge operations is generally made by followingexisting recipes. In this paper, we conduct a systematic exploration ofdifferent BPE merge operations to understand how it interacts with the modelarchitecture, the strategy to build vocabularies and the language pair. Ourexploration could provide guidance for selecting proper BPE configurations inthe future. Most prominently: we show that for LSTM-based architectures, it isnecessary to experiment with a wide range of different BPE operations as thereis no typical optimal BPE configuration, whereas for Transformer architectures,smaller BPE size tends to be a typically optimal choice. We urge the communityto make prudent choices with subword merge operations, as our experimentsindicate that a sub-optimal BPE configuration alone could easily reduce thesystem performance by 3-4 BLEU points.
 
【24】 What Syntactic Structures block Dependencies in RNN Language Models?
标题：什么句法结构阻止RNN语言模型中的依赖？
作者： Ethan Wilcox,  Richard Futrell 
备注：To Appear at the 41st Annual Meeting of the Cognitive Science Society, Montreal, Canada, July 2019
链接：https://arxiv.org/abs/1905.10431
摘要： Recurrent Neural Networks (RNNs) trained on a language modeling task havebeen shown to acquire a number of non-local grammatical dependencies with somesuccess. Here, we provide new evidence that RNN language models are sensitiveto hierarchical syntactic structure by investigating the filler--gap dependencyand constraints on it, known as syntactic islands. Previous work isinconclusive about whether RNNs learn to attenuate their expectations for gapsin island constructions in particular or in any sufficiently complex syntacticenvironment. This paper gives new evidence for the former by providing controlstudies that have been lacking so far. We demonstrate that two state-of-the-artRNN models are are able to maintain the filler--gap dependency throughunbounded sentential embeddings and are also sensitive to the hierarchicalrelationship between the filler and the gap. Next, we demonstrate that themodels are able to maintain possessive pronoun gender expectations throughisland constructions---this control case rules out the possibility that islandconstructions block all information flow in these networks. We also evaluatethree untested islands constraints: coordination islands, left branch islands,and sentential subject islands. Models are able to learn left branch islandsand learn coordination islands gradiently, but fail to learn sentential subjectislands. Through these controls and new tests, we provide evidence that modelbehavior is due to finer-grained expectations than gross syntactic complexity,but also that the models are conspicuously un-humanlike in some of theirperformance characteristics.
 
【25】 Human vs. Muppet: A Conservative Estimate of HumanPerformance on the  GLUE Benchmark
标题：Human vs. Muppet：GLUE基准上对人类表现的保守估计
作者： Nikita Nangia,  Samuel R. Bowman 
链接：https://arxiv.org/abs/1905.10425
摘要： The GLUE benchmark (Wang et al., 2019b) is a suite of language understandingtasks which has seen dramatic progress in the past year, with averageperformance moving from 70.0 at launch to 83.9, state of the art at the time ofwriting (May 24, 2019). Here, we measure human performance on the benchmark, inorder to learn whether significant headroom remains for further progress. Weprovide a conservative estimate of human performance on the benchmark throughcrowdsourcing: Our annotators are non-experts who must learn each task from abrief set of instructions and 20 examples. In spite of limited training, theseannotators robustly outperform the state of the art on six of the nine GLUEtasks and achieve an average score of 87.1. Given the fast pace of progresshowever, the headroom we observe is quite limited. To reproduce the data-poorsetting that our annotators must learn in, we also train the BERT model (Devlinet al., 2019) in limited-data regimes, and conclude that low-resource sentenceclassification remains a challenge for modern neural network approaches to textunderstanding.
 
【26】 Using Deep Networks and Transfer Learning to Address Disinformation
标题：使用深度网络和转移学习来解决信息
作者： Numa Dhamani,  Jonathon Morgan 
备注：AI for Social Good Workshop at the International Conference on Machine Learning, Long Beach, United States (2019)
链接：https://arxiv.org/abs/1905.10412
摘要： We apply an ensemble pipeline composed of a character-level convolutionalneural network (CNN) and a long short-term memory (LSTM) as a general tool foraddressing a range of disinformation problems. We also demonstrate the abilityto use this architecture to transfer knowledge from labeled data in one domainto related (supervised and unsupervised) tasks. Character-level neural networksand transfer learning are particularly valuable tools in the disinformationspace because of the messy nature of social media, lack of labeled data, andthe multi-channel tactics of influence campaigns. We demonstrate theireffectiveness in several tasks relevant for detecting disinformation: spamemails, review bombing, political sentiment, and conversation clustering.
 
【27】 EG-GAN: Cross-Language Emotion Gain Synthesis based on Cycle-Consistent  Adversarial Networks
标题：EG-GAN：基于周期一致的对抗网络的跨语言情感增益综合
作者： Xiaoqi Jia,  Haichao Du 
链接：https://arxiv.org/abs/1905.11173
摘要： Despite remarkable contributions from existing emotional speech synthesizers,we find that these methods are based on Text-to-Speech system or limited byaligned speech pairs, which suffered from pure emotion gain synthesis.Meanwhile, few studies have discussed the cross-language generalization abilityof above methods to cope with the task of emotional speech synthesis in variouslanguages. We propose a cross-language emotion gain synthesis method namedEG-GAN which can learn a language-independent mapping from source emotiondomain to target emotion domain in the absence of paired speech samples. EG-GANis based on cycle-consistent generation adversarial network with a gradientpenalty and an auxiliary speaker discriminator. The domain adaptation isintroduced to implement the rapid migrating and sharing of emotional gainsamong different languages. The experiment results show that our method canefficiently synthesize high quality emotional speech from any source speech forgiven emotion categories, without the limitation of language differences andaligned speech pairs.
 
【28】 An Empirical Study on Post-processing Methods for Word Embeddings
标题：词嵌入后处理方法的实证研究
作者： Shuai Tang,  Virginia R. de Sa 
链接：https://arxiv.org/abs/1905.10971
摘要： Word embeddings learnt from large corpora have been adopted in variousapplications in natural language processing and served as the general inputrepresentations to learning systems. Recently, a series of post-processingmethods have been proposed to boost the performance of word embeddings onsimilarity comparison and analogy retrieval tasks, and some have been adaptedto compose sentence representations. The general hypothesis behind thesemethods is that by enforcing the embedding space to be more isotropic, thesimilarity between words can be better expressed. We view these methods as anapproach to shrink the covariance/gram matrix, which is estimated by learningword vectors, towards a scaled identity matrix. By optimising an objective inthe semi-Riemannian manifold with Centralised Kernel Alignment (CKA), we areable to search for the optimal shrinkage parameter, and provide apost-processing method to smooth the spectrum of learnt word vectors whichyields improved performance on downstream tasks.
 
【29】 Enhancing Item Response Theory for Cognitive Diagnosis
标题：增强认知诊断的项目反应理论
作者： Song Cheng,  Qi Liu 
链接：https://arxiv.org/abs/1905.10957
摘要： Cognitive diagnosis is a fundamental and crucial task in many educationalapplications, e.g., computer adaptive test and cognitive assignments. ItemResponse Theory (IRT) is a classical cognitive diagnosis method which canprovide interpretable parameters (i.e., student latent trait, questiondiscrimination, and difficulty) for analyzing student performance. However,traditional IRT ignores the rich information in question texts, cannot diagnoseknowledge concept proficiency, and it is inaccurate to diagnose the parametersfor the questions which only appear several times. To this end, in this paper,we propose a general Deep Item Response Theory (DIRT) framework to enhancetraditional IRT for cognitive diagnosis by exploiting semantic representationfrom question texts with deep learning. In DIRT, we first use a proficiencyvector to represent students' proficiency in knowledge concepts and embedquestion texts and knowledge concepts to dense vectors by Word2Vec. Then, wedesign a deep diagnosis module to diagnose parameters in traditional IRT bydeep learning techniques. Finally, with the diagnosed parameters, we input theminto the logistic-like formula of IRT to predict student performance. Extensiveexperimental results on real-world data clearly demonstrate the effectivenessand interpretation power of DIRT framework.
 
【30】 QuesNet: A Unified Representation for Heterogeneous Test Questions
标题：QuesNet：异构测试问题的统一表示
作者： Yu Yin,  Yu Su 
链接：https://arxiv.org/abs/1905.10949
摘要： Understanding learning materials (e.g. test questions) is a crucial issue inonline learning systems, which can promote many applications in educationdomain. Unfortunately, many supervised approaches suffer from the problem ofscarce human labeled data, whereas abundant unlabeled resources are highlyunderutilized. To alleviate this problem, an effective solution is to usepre-trained representations for question understanding. However, existingpre-training methods in NLP area are infeasible to learn test questionrepresentations due to several domain-specific characteristics in education.First, questions usually comprise of heterogeneous data including content text,images and side information. Second, there exists both basic linguisticinformation as well as domain logic and knowledge. To this end, in this paper,we propose a novel pre-training method, namely QuesNet, for comprehensivelylearning question representations. Specifically, we first design a unifiedframework to aggregate question information with its heterogeneous inputs intoa comprehensive vector. Then we propose a two-level hierarchical pre-trainingalgorithm to learn better understanding of test questions in an unsupervisedway. Here, a novel holed language model objective is developed to extractlow-level linguistic features, and a domain-oriented objective is proposed tolearn high-level logic and knowledge. Moreover, we show that QuesNet has goodcapability of being fine-tuned in many question-based tasks. We conductextensive experiments on large-scale real-world question data, where theexperimental results clearly demonstrate the effectiveness of QuesNet forquestion understanding as well as its superior applicability.
 
【31】 Sequential Graph Dependency Parser
标题：顺序图依赖性解析器
作者： Sean Welleck,  Kyunghyun Cho 
链接：https://arxiv.org/abs/1905.10930
摘要： We propose a method for non-projective dependency parsing by incrementallypredicting a set of edges. Since the edges do not have a pre-specified order,we propose a set-based learning method. Our method blends graph, transition,and easy-first parsing, including a prior state of the parser as a specialcase. The proposed transition-based method successfully parses near the stateof the art on both projective and non-projective languages, without assuming acertain parsing order.
 
【32】 Hyperbolic Interaction Model For Hierarchical Multi-Label Classification
标题：分层多标签分类的双曲线交互模型
作者： Boli Chen,  Liping Jing 
链接：https://arxiv.org/abs/1905.10802
摘要： Different from the traditional classification tasks which assume mutualexclusion of labels, hierarchical multi-label classification (HMLC) aims toassign multiple labels to every instance with the labels organized underhierarchical relations. In fact, linguistic ontologies are intrinsichierarchies. Besides the labels, the conceptual relations between words canalso form hierarchical structures. Thus it can be a challenge to learn mappingsfrom the word space to the label space, and vice versa. We propose to model theword and label hierarchies by embedding them jointly in the hyperbolic space.The main reason is that the tree-likeness of the hyperbolic space matches thecomplexity of symbolic data with hierarchical structures. A new hyperbolicinteraction model (HyperIM) is designed to learn the label-aware documentrepresentations and make predictions for HMLC. Extensive experiments areconducted on three benchmark datasets. The results have demonstrated that thenew model can realistically capture the complex data structures and furtherimprove the performance for HMLC comparing with the state-of-the-art methods.To facilitate future research, our code is publicly available.
 
【33】 Path Ranking with Attention to Type Hierarchies
标题：注意类型层次结构的路径排名
作者： Weiyu Liu,  Sonia Chernova 
链接：https://arxiv.org/abs/1905.10799
摘要： The knowledge base completion problem is the problem of inferring missinginformation from existing facts in knowledge bases. Path-ranking based methodsuse sequences of relations as general patterns of paths for prediction.However, these patterns usually lack accuracy because they are generic and canoften apply to widely varying scenarios. We leverage type hierarchies ofentities to create a new class of path patterns that are both discriminativeand generalizable. Then we propose an attention-based RNN model, which can betrained end-to-end, to discover the new path patterns most suitable for thedata. Experiments conducted on two benchmark knowledge base completion datasetsdemonstrate that the proposed model outperforms existing methods by astatistically significant margin. Our quantitative analysis of the pathpatterns shows that they balance between generalization and discrimination.
 
【34】 MDE: Multi Distance Embeddings for Link Prediction in Knowledge Graphs
标题：MDE：用于知识图中链接预测的多距离嵌入
作者： Afshin Sadeghi,  Jens Lehmann 
链接：https://arxiv.org/abs/1905.10702
摘要： Over the past decade, knowledge graphs became popular for capturingstructured domain knowledge. Relational learning models enable the predictionof missing links inside knowledge graphs. More specifically, latent distanceapproaches model the relationships among entities via a distance between latentrepresentations. Translating embedding models (e.g., TransE) are among the mostpopular latent distance approaches which use one distance function to learnmultiple relation patterns. However, they are not capable of capturingsymmetric relations. They also force relations with reflexive patterns tobecome symmetric and transitive. In order to improve distance based embedding,we propose multi-distance embeddings (MDE). Our solution is based on the ideathat by learning independent embedding vectors for each entity and relation onecan aggregate contrasting distance functions. Benefiting from MDE, we alsodevelop supplementary distances resolving the above-mentioned limitations ofTransE. We further propose an extended loss function for distance basedembeddings and show that MDE and TransE are fully expressive using this lossfunction. Furthermore, we obtain a bound on the size of their embeddings forfull expressivity. Our empirical results show that MDE significantly improvesthe translating embeddings and outperforms several state-of-the-art embeddingmodels on benchmark datasets.
 
【35】 Quantifying Exposure Bias for Neural Language Generation
标题：量化神经语言生成的暴露偏差
作者： Tianxing He,  James Glass 
链接：https://arxiv.org/abs/1905.10617
摘要： The exposure bias problem refers to the training-inference discrepancy causedby teacher forcing in maximum likelihood estimation (MLE) training forrecurrent neural network language models (RNNLM). It has been regarded as acentral problem for natural language generation (NLG) model training. Althougha lot of algorithms have been proposed to avoid teacher forcing and thereforeto remove exposure bias, there is little work showing how serious the exposurebias problem is. In this work, starting from the definition of exposure bias,we propose two simple and intuitive approaches to quantify exposure bias forMLE-trained language models. Experiments are conducted on both synthetic andreal data-sets. Surprisingly, our results indicate that either exposure bias istrivial (i.e. indistinguishable from the mismatch between model and datadistribution), or is not as significant as it is presumed to be (with ameasured performance gap of 3%). With this work, we suggest re-evaluating theviewpoint that teacher forcing or exposure bias is a major drawback of MLEtraining.
 
【36】 Differentiable Representations For Multihop Inference Rules
标题：多跳推理规则的可区分表示
作者： William W. Cohen,  Matthew Siegler 
链接：https://arxiv.org/abs/1905.10417
摘要： We present efficient differentiable implementations of second-order multi-hopreasoning using a large symbolic knowledge base (KB). We introduce a newoperation which can be used to compositionally construct second-order multi-hoptemplates in a neural model, and evaluate a number of alternativeimplementations, with different time and memory trade offs. These techniquesscale to KBs with millions of entities and tens of millions of triples, andlead to simple models with competitive performance on several learning tasksrequiring multi-hop reasoning.
 
翻译：谷歌翻译
